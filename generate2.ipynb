{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_sim import LlamaForCausalLM\n",
    "#from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "hf_llm = LlamaForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-70B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)\n",
    "hf_llm = hf_llm.eval()\n",
    "hf_llm.config.K = 12\n",
    "hf_llm.config.L = 150\n",
    "hf_llm.config.cache_mode = \"anns_es\"\n",
    "hf_llm.config.window = 16\n",
    "hf_llm.set_sparse_attn(sparse=0.02, random_sparse=0.2, window_size=16, kernel_size=5)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-70B-Instruct\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "System: You are an assistant. Your goal is to be helpful, friendly and answer the questions as more as you can. \n",
    "User: Hello, I want to test your mathematic ability, nice to meet you! I have a problem for you.\n",
    "@<<<assign v0 = v2 - 1>>>@@<<<assign v1 = v3 + 1>>>@@<<<assign v2 = 1>>>@@<<<assign v3 = v2>>>@\n",
    "**Question:**\n",
    "Can you tell me which variables are equal to 2 in those assignments in '<<<>>>'? The values of those variables can be used before definition as they are constant.\n",
    "You can solve the problem in a step-by-step way.\n",
    "Assistant: \n",
    "\"\"\"\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(input_tokens, len(input_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = hf_llm.generate(inputs=input_tokens, do_sample=False, max_new_tokens=1024)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
